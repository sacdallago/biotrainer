{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This notebook demonstrates how to concatenate two embeddings (on sequence level).\n",
    "# We concatenate one_hot_encoding and word2vec embeddings.\n",
    "# The sequences have ids [Seq1, Seq2, Seq3, Seq4], like the example for sequence_to_class\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Load existing embedding files\n",
    "embeddings_one_hot_path = \"reduced_embeddings_file_one_hot_encoding.h5\"\n",
    "embeddings_word2vec_path = \"reduced_embeddings_file_word2vec.h5\"\n",
    "\n",
    "embeddings_one_hot_file = h5py.File(embeddings_one_hot_path, 'r', rdcc_nbytes=1024 ** 2 * 4000, rdcc_nslots=1e7)\n",
    "embeddings_word2vec_file = h5py.File(embeddings_word2vec_path, 'r', rdcc_nbytes=1024 ** 2 * 4000, rdcc_nslots=1e7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Merge embedding files:\n",
    "# 1. Create a new file\n",
    "output_embeddings_path = \"reduced_one_hot_and_word2vec_embeddings.h5\"\n",
    "# 2. Get embedding dimensions to merge\n",
    "one_hot_dim = embeddings_one_hot_file[\"0\"].shape[0]\n",
    "word2vec_dim = embeddings_word2vec_file[\"0\"].shape[0]\n",
    "\n",
    "with h5py.File(output_embeddings_path, \"w\") as output_embeddings_file:\n",
    "    # 3. Save one_hot_encoding values in new file with extended shape\n",
    "    for idx, embedding in embeddings_one_hot_file.items():\n",
    "        output_embeddings_file.create_dataset(idx, data=embedding, compression=\"gzip\", chunks=True,\n",
    "                                      maxshape=(one_hot_dim + word2vec_dim))\n",
    "        output_embeddings_file[idx].attrs[\"original_id\"] = embeddings_one_hot_file[idx].attrs[\"original_id\"]\n",
    "\n",
    "    # 4. Append word2vec embeddings\n",
    "    for idx, embedding in output_embeddings_file.items():\n",
    "        appendix = embeddings_word2vec_file[idx]\n",
    "        output_embeddings_file[idx].resize((one_hot_dim + word2vec_dim), axis=0)\n",
    "        output_embeddings_file[idx][-appendix.size:] = appendix\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verify merged file\n",
    "combined_embeddings_file = h5py.File(output_embeddings_path, 'r', rdcc_nbytes=1024 ** 2 * 4000, rdcc_nslots=1e7)\n",
    "\n",
    "for idx, embedding in combined_embeddings_file.items():\n",
    "    assert embedding.shape[0] == one_hot_dim + word2vec_dim, \"New dimension is not correct\"\n",
    "    assert not (embedding[:one_hot_dim] - embeddings_one_hot_file[idx]).all(), \"One_hot_encodings not correctly merged\"\n",
    "    assert not (embedding[one_hot_dim:] - embeddings_word2vec_file[idx]).all(), \"Word2vec not correctly merged\"\n",
    "\n",
    "# Show embeddings in internal biotrainer format\n",
    "id2emb = {combined_embeddings_file[idx].attrs[\"original_id\"]: embedding for (idx, embedding) in\n",
    "          combined_embeddings_file.items()}\n",
    "print(\"{ID: Embedding} in biotrainer format:\\n\", id2emb)\n",
    "\n",
    "combined_embeddings_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}