{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This notebook demonstrates how to concatenate two embeddings.\n",
    "# At first, we will use embeddings on a sequence level (reduced embeddings), secondly we will concatenate embeddings on a per-residue level.\n",
    "# We concatenate one_hot_encoding and word2vec embeddings.\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'I. SEQUENCE LEVEL EMBEDDINGS (reduced)'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"I. SEQUENCE LEVEL EMBEDDINGS (reduced)\"\"\"\n",
    "# The sequences have ids [Seq1, Seq2, Seq3, Seq4], like the example for sequence_to_class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load existing reduced embedding files\n",
    "reduced_embeddings_one_hot_path = \"../example_files/reduced_embeddings_file_one_hot_encoding.h5\"\n",
    "reduced_embeddings_word2vec_path = \"../example_files/reduced_embeddings_file_word2vec.h5\"\n",
    "\n",
    "reduced_embeddings_one_hot_file = h5py.File(reduced_embeddings_one_hot_path, 'r', rdcc_nbytes=1024 ** 2 * 4000, rdcc_nslots=1e7)\n",
    "reduced_embeddings_word2vec_file = h5py.File(reduced_embeddings_word2vec_path, 'r', rdcc_nbytes=1024 ** 2 * 4000, rdcc_nslots=1e7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Merge embedding files:\n",
    "# 1. Create a new file\n",
    "reduced_output_embeddings_path = \"reduced_one_hot_and_word2vec_embeddings.h5\"\n",
    "# 2. Get embedding dimensions to merge\n",
    "one_hot_dim = reduced_embeddings_one_hot_file[\"0\"].shape[0]\n",
    "word2vec_dim = reduced_embeddings_word2vec_file[\"0\"].shape[0]\n",
    "\n",
    "with h5py.File(reduced_output_embeddings_path, \"w\") as reduced_output_embeddings_file:\n",
    "    # 3. Save one_hot_encoding values in new file with extended shape\n",
    "    for idx, embedding in reduced_embeddings_one_hot_file.items():\n",
    "        reduced_output_embeddings_file.create_dataset(idx, data=embedding, compression=\"gzip\", chunks=True,\n",
    "                                      maxshape=(one_hot_dim + word2vec_dim))\n",
    "        reduced_output_embeddings_file[idx].attrs[\"original_id\"] = reduced_embeddings_one_hot_file[idx].attrs[\"original_id\"]\n",
    "\n",
    "    # 4. Append word2vec embeddings\n",
    "    for idx, embedding in reduced_output_embeddings_file.items():\n",
    "        appendix = reduced_embeddings_word2vec_file[idx]\n",
    "        reduced_output_embeddings_file[idx].resize((one_hot_dim + word2vec_dim), axis=0)\n",
    "        reduced_output_embeddings_file[idx][-appendix.size:] = appendix\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ID: Embedding} in biotrainer format:\n",
      " {'Seq3': <HDF5 dataset \"0\": shape (533,), type \"<f4\">, 'Seq4': <HDF5 dataset \"1\": shape (533,), type \"<f4\">, 'Seq1': <HDF5 dataset \"2\": shape (533,), type \"<f4\">, 'Seq2': <HDF5 dataset \"3\": shape (533,), type \"<f4\">}\n"
     ]
    }
   ],
   "source": [
    "# Verify merged file\n",
    "reduced_combined_embeddings_file = h5py.File(reduced_output_embeddings_path, 'r', rdcc_nbytes=1024 ** 2 * 4000, rdcc_nslots=1e7)\n",
    "\n",
    "for idx, embedding in reduced_combined_embeddings_file.items():\n",
    "    assert embedding.shape[0] == one_hot_dim + word2vec_dim, \"New dimension is not correct\"\n",
    "    assert not (embedding[:one_hot_dim] - reduced_embeddings_one_hot_file[idx]).all(), \"One_hot_encodings not correctly merged\"\n",
    "    assert not (embedding[one_hot_dim:] - reduced_embeddings_word2vec_file[idx]).all(), \"Word2vec not correctly merged\"\n",
    "\n",
    "# Show embeddings in internal biotrainer format\n",
    "id2emb = {reduced_combined_embeddings_file[idx].attrs[\"original_id\"]: embedding for (idx, embedding) in\n",
    "          reduced_combined_embeddings_file.items()}\n",
    "print(\"{ID: Embedding} in biotrainer format:\\n\", id2emb)\n",
    "\n",
    "reduced_combined_embeddings_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'II. RESIDUE LEVEL EMBEDDINGS '"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"II. RESIDUE LEVEL EMBEDDINGS \"\"\"\n",
    "# The sequences have ids [Seq1, Seq2, Seq3], like the example for residue_to_class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load existing embedding files\n",
    "embeddings_one_hot_path = \"../example_files/embeddings_file_one_hot_encoding.h5\"\n",
    "embeddings_word2vec_path = \"../example_files/embeddings_file_word2vec.h5\"\n",
    "\n",
    "embeddings_one_hot_file = h5py.File(embeddings_one_hot_path, 'r', rdcc_nbytes=1024 ** 2 * 4000, rdcc_nslots=1e7)\n",
    "embeddings_word2vec_file = h5py.File(embeddings_word2vec_path, 'r', rdcc_nbytes=1024 ** 2 * 4000, rdcc_nslots=1e7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Merge embedding files:\n",
    "# 1. Create a new file\n",
    "output_embeddings_path = \"one_hot_and_word2vec_embeddings.h5\"\n",
    "# 2. Get embedding dimensions to merge\n",
    "one_hot_dim = embeddings_one_hot_file[\"0\"].shape[1]\n",
    "word2vec_dim = embeddings_word2vec_file[\"0\"].shape[1]\n",
    "\n",
    "with h5py.File(output_embeddings_path, \"w\") as output_embeddings_file:\n",
    "    # 3. Save one_hot_encoding values in new file with extended shape\n",
    "    for idx, embedding in embeddings_one_hot_file.items():\n",
    "        sequence_length = embedding.shape[0]\n",
    "        output_embeddings_file.create_dataset(idx, data=embedding, compression=\"gzip\", chunks=True,\n",
    "                                      maxshape=(sequence_length, one_hot_dim + word2vec_dim))\n",
    "        output_embeddings_file[idx].attrs[\"original_id\"] = embeddings_one_hot_file[idx].attrs[\"original_id\"]\n",
    "\n",
    "    # 4. Append word2vec embeddings\n",
    "    for idx, embedding in output_embeddings_file.items():\n",
    "        output_embeddings_file[idx].resize((one_hot_dim + word2vec_dim), axis=1)\n",
    "        for residue in range(output_embeddings_file[idx].shape[0]):\n",
    "            appendix = embeddings_word2vec_file[idx][residue]\n",
    "            output_embeddings_file[idx][residue, -appendix.size:] = appendix\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ID: Embedding} in biotrainer format:\n",
      " {'Seq3': <HDF5 dataset \"0\": shape (14, 533), type \"<f4\">, 'Seq1': <HDF5 dataset \"1\": shape (8, 533), type \"<f4\">, 'Seq2': <HDF5 dataset \"2\": shape (6, 533), type \"<f4\">}\n"
     ]
    }
   ],
   "source": [
    "# Verify merged file\n",
    "combined_embeddings_file = h5py.File(output_embeddings_path, 'r', rdcc_nbytes=1024 ** 2 * 4000, rdcc_nslots=1e7)\n",
    "\n",
    "for idx, embedding in combined_embeddings_file.items():\n",
    "    assert embedding.shape[1] == one_hot_dim + word2vec_dim, \"New dimension is not correct\"\n",
    "    for residue in range(embedding.shape[0]):\n",
    "        assert not (embedding[residue][:one_hot_dim] - embeddings_one_hot_file[idx][residue]).all(), \"One_hot_encodings not correctly merged\"\n",
    "        assert not (embedding[residue][one_hot_dim:] - embeddings_word2vec_file[idx][residue]).all(), \"Word2vec not correctly merged\"\n",
    "\n",
    "# Show embeddings in internal biotrainer format\n",
    "id2emb = {combined_embeddings_file[idx].attrs[\"original_id\"]: embedding for (idx, embedding) in\n",
    "          combined_embeddings_file.items()}\n",
    "print(\"{ID: Embedding} in biotrainer format:\\n\", id2emb)\n",
    "\n",
    "combined_embeddings_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}