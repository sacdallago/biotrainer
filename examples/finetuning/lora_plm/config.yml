input_file: sequences.fasta
protocol: residue_to_class
model_choice: FNN
optimizer_choice: adam
loss_choice: cross_entropy_loss
num_epochs: 10
use_class_weights: True
learning_rate: 1e-3
batch_size: 128
save_split_ids: False
device: cuda
disable_pytorch_compile: True
bootstrapping_iterations: 30
external_writer: none
finetuning_config:
  method: lora
  random_masking: True  # Enable Masked Language Modeling (MLM)
  lora_r: 8  # Lora Rank
  lora_alpha: 16  # Lora Alpha
  lora_dropout: 0.05  # Lora dropout probability
  lora_target_modules: ["q", "k", "v", "o"]  # Names of target modules for lora (ProtT5), ESM: ["query", "key", "value"]
  lora_bias: lora_only  # Bias type for lora
embedder_name: Rostlab/prot_t5_xl_uniref50