{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Using biotrainer autoeval for plm evaluation\n",
    "\n",
    "This notebook shows an example how to use the biotrainer `autoeval` module for automatic plm evaluation. We use the [PBC](https://github.com/Rostlab/pbc) framework that includes curated datasets that are established for plm benchmarking."
   ],
   "id": "412c00070a21e58d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install biotrainer if you haven't\n",
    "# !pip install biotrainer"
   ],
   "id": "bcec39642cbdeec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Default Use Case: Model Download from Huggingface\n",
    "\n",
    "The most convient option to use the biotrainer autoeval pipeline is to use the huggingface id of your plm. This will automatically download the model, calculate the embeddings and run the evaluation."
   ],
   "id": "ec2565addb07ba1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Define variables\n",
    "embedder_name = \"Rostlab/prot_t5_xl_uniref50\"  # Replace with your plm's huggingface id. For alternatives, see \"Advanced options\" below\n",
    "framework = \"pbc\"\n",
    "min_seq_length = 0  # Default\n",
    "max_seq_length = 2000  # Default"
   ],
   "id": "796a1990e3834cd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the pipeline\n",
    "from biotrainer.autoeval import autoeval_pipeline\n",
    "\n",
    "current_progress = None\n",
    "for progress in autoeval_pipeline(embedder_name=embedder_name,\n",
    "                                  framework=framework,\n",
    "                                  min_seq_length=min_seq_length,\n",
    "                                  max_seq_length=max_seq_length):\n",
    "    print(progress)  # The pipeline is a generator function to inform the user about the current progress.\n",
    "    current_progress = progress"
   ],
   "id": "79c786f5f67fd72f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's look at the results\n",
    "if current_progress is None or current_progress.final_report is None:\n",
    "    print(\"No results found.\")  # Something went wrong\n",
    "else:\n",
    "    final_report = current_progress.final_report\n",
    "    scl_result = final_report[\"results\"][\"PBC-scl\"][\"test_results\"]['test']['metrics']['accuracy']\n",
    "    sec_struct_result_newpisces364 = final_report[\"results\"][\"PBC-secondary_structure\"][\"test_results\"]['newpisces364']['metrics']['accuracy']\n",
    "    sec_struct_result_casp12 = final_report[\"results\"][\"PBC-secondary_structure\"][\"test_results\"]['casp12']['metrics']['accuracy']\n",
    "    sec_struct_result_casp13 = final_report[\"results\"][\"PBC-secondary_structure\"][\"test_results\"]['casp13']['metrics']['accuracy']\n",
    "    sec_struct_result_casp14 = final_report[\"results\"][\"PBC-secondary_structure\"][\"test_results\"]['casp13']['metrics']['accuracy']\n",
    "\n",
    "    print(f\"PBC-scl results: {scl_results} (accuracy on test)\\n\")\n",
    "    print(f\"PBC-secondary_structure results:\")\n",
    "    print(f\"newpisces364: {sec_struct_result_newpisces364} (accuracy)\")\n",
    "    print(f\"casp12: {sec_struct_result_casp12} (accuracy)\")\n",
    "    print(f\"casp13: {sec_struct_result_casp13} (accuracy)\")\n",
    "    print(f\"casp14: {sec_struct_result_casp14} (accuracy)\")"
   ],
   "id": "7a65cc669cc48b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The full report file can be found at `autoeval_output/{embedder_name}/autoeval_report_{embedder_name}.json`",
   "id": "8c6bc84a1c0096de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Advanced options 1: Using a custom embedding function\n",
    "\n",
    "If you are running biotrainer-autoeval directly after training your model, the model will probably not be available on huggingface, but locally. Therefore, you can provide custom embedding functions both for per-sequence and per-residue embeddings to be independent of the biotrainer embedding module. The provided functions take a list of strings (sequences) as input and must return, for each sequence, the sequence and the respective embedding. This is to ensure that the sequence is always mapped to the correct embedding.\n",
    "\n",
    "*What is a generator function?*:\n",
    "\n",
    "A generator function returns a result as soon as it is available, and only continues to create new results after the previous one has been processed. In this case, this is useful because it allows to save the embeddings after computation, thus avoiding that the RAM runs full with the embeddings."
   ],
   "id": "462161ef9c176b65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Abstract Explanation\n",
    "custom_embedding_function_per_sequence = lambda seq: (seq, torch.empty())  # Define your function as a generator here\n",
    "custom_embedding_function_per_residue = lambda seq: (seq, torch.empty())  # Define your function as a generator here\n",
    "for progress in autoeval_pipeline(embedder_name=embedder_name,\n",
    "                                  framework=framework,\n",
    "                                  custom_embedding_function_per_sequence=custom_embedding_function_per_sequence,\n",
    "                                  custom_embedding_function_per_residue=custom_embedding_function_per_residue,\n",
    "                                  min_seq_length=min_seq_length,\n",
    "                                  max_seq_length=max_seq_length):\n",
    "    print(progress)  # The pipeline is a generator function to inform the user about the current progress."
   ],
   "id": "e4fc5ae4bab03f07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Concrete Example with the ProtT5 QuickStart Tutorial\n",
    "\n",
    "Now we use the [ProtT5 QuickStart Tutorial](https://github.com/agemagician/ProtTrans?tab=readme-ov-file#-quick-start) to show how to implement ProtT5 via the custom_embedding_functions into autoeval. Note that this example does not use batching efficiently, but it is a good starting point for your own implementation:"
   ],
   "id": "9ad8dd2ebee1654"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "device = 'cpu' #torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "if device == torch.device(\"cpu\"):\n",
    "    model.to(torch.float32)\n",
    "\n",
    "def embed_per_residue(sequences: list):\n",
    "    for sequence in sequences:\n",
    "        # replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "        sequence_cleaned = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))]\n",
    "        ids = tokenizer(sequence_cleaned, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "        input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "        # generate embeddings\n",
    "        with torch.no_grad():\n",
    "            embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embedding = embedding_repr.last_hidden_state[0,:len(sequence)]\n",
    "        yield sequence, embedding\n",
    "\n",
    "def embed_per_sequence(sequences: str):\n",
    "    for sequence in sequences:\n",
    "        _, embedding = embed_per_residue(sequence)\n",
    "        yield sequence, embedding.mean(dim=0) # shape (1024)\n",
    "\n",
    "# Run Autoeval\n",
    "for progress in autoeval_pipeline(embedder_name=\"ProtT5-custom\",\n",
    "                                  framework=framework,\n",
    "                                  custom_embedding_function_per_sequence=embed_per_residue,\n",
    "                                  custom_embedding_function_per_residue=embed_per_sequence,\n",
    "                                  min_seq_length=min_seq_length,\n",
    "                                  max_seq_length=max_seq_length):\n",
    "    print(progress)  # The pipeline is a generator function to inform the user about the current progress."
   ],
   "id": "58faf8fc0e878dfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Advanced Options 2: Precomputed embeddings file\n",
    "\n",
    "Another option is to use precomputed embeddings file, if you prefer that or have them already. Just make sure that the files include embeddings for all framework sequences and are stored by sequence hash, according to biotrainer standards."
   ],
   "id": "9c544f5ecf9fb136"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from biotrainer.autoeval import get_unique_framework_sequences\n",
    "\n",
    "_, per_residue_seqs, per_sequence_seqs = get_unique_framework_sequences(framework=framework,\n",
    "                                                                        min_seq_length=min_seq_length,\n",
    "                                                                        max_seq_length=max_seq_length)\n",
    "# per_residue_seqs and per_sequence_seqs are dictionaries mapping sequence hashes to BiotrainerSequenceRecord objects, use that hash as an id when storing your embeddings\n",
    "\n",
    "per_residue_path = Path()  # TODO Your per-residue embeddings path\n",
    "per_sequence_path = Path()  # TODO Your per-sequence embeddings path\n",
    "for progress in autoeval_pipeline(embedder_name=embedder_name,\n",
    "                                  framework=framework,\n",
    "                                  precomputed_per_residue_embeddings=per_residue_path,\n",
    "                                  precomputed_per_sequence_embeddings=per_sequence_path,\n",
    "                                  min_seq_length=min_seq_length,\n",
    "                                  max_seq_length=max_seq_length):\n",
    "    print(progress)"
   ],
   "id": "40388d81df59747",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
