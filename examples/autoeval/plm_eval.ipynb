{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Using biotrainer-autoeval for plm evaluation\n",
    "\n",
    "This notebook shows an example how to use the biotrainer `autoeval` module for automatic plm evaluation. We use the [DWT](https://github.com/Rostlab/dwt) framework that includes curated datasets that are well interpretable for plm benchmarking."
   ],
   "id": "412c00070a21e58d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install biotrainer if you haven't\n",
    "# !pip install biotrainer"
   ],
   "id": "bcec39642cbdeec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Define variables\n",
    "embedder_name = \"Rostlab/prot_t5_xl_uniref50\"  # Replace with your plm's huggingface id. For alternatives, see \"Advanced options\" below\n",
    "framework = \"dwt\"\n",
    "min_seq_length = 0  # Default\n",
    "max_seq_length = 2000  # Default"
   ],
   "id": "796a1990e3834cd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the pipeline\n",
    "from biotrainer.autoeval import autoeval_pipeline\n",
    "\n",
    "current_progress = None\n",
    "for progress in autoeval_pipeline(embedder_name=\"one_hot_encoding\",\n",
    "                                  framework=framework,\n",
    "                                  min_seq_length=min_seq_length,\n",
    "                                  max_seq_length=max_seq_length):\n",
    "    print(progress)  # The pipeline is a generator function to inform the user about the current progress.\n",
    "    current_progress = progress"
   ],
   "id": "79c786f5f67fd72f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's look at the results\n",
    "if current_progress is None or current_progress.final_report is None:\n",
    "    print(\"No results found.\")  # Something went wrong\n",
    "else:\n",
    "    final_report = current_progress.final_report\n",
    "    scl_results = final_report[\"results\"][\"DWT-scl\"][\"test_results\"]\n",
    "    sec_struct_results = final_report[\"results\"][\"DWT-secondary_structure\"][\"test_results\"]\n",
    "\n",
    "    print(f\"DWT-scl results: {scl_results}\\n\")\n",
    "    print(f\"DWT-secondary_structure results: {sec_struct_results}\\n\")"
   ],
   "id": "7a65cc669cc48b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cfedc62291162c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Advanced options 1: Using a custom embedding function\n",
    "\n",
    "If you are running biotrainer-autoeval directly after training your model, the model will probably not be available on huggingface, but locally. Therefore, you can provide custom embedding functions both for per-sequence and per-residue embeddings to be independent of the biotrainer embedding module. The provided functions take a list of strings (sequences) as input and must return, for each sequence, the sequence and the respective embedding. This is to ensure that the sequence is always mapped to the correct embedding."
   ],
   "id": "462161ef9c176b65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "custom_embedding_function_per_sequence = lambda seq: (seq, torch.empty())  # Define your function as a generator here\n",
    "custom_embedding_function_per_residue = lambda seq: (seq, torch.empty())  # Define your function as a generator here\n",
    "for progress in autoeval_pipeline(embedder_name=embedder_name,\n",
    "                                  framework=framework,\n",
    "                                  custom_embedding_function_per_sequence=custom_embedding_function_per_sequence,\n",
    "                                  custom_embedding_function_per_residue=custom_embedding_function_per_residue,\n",
    "                                  min_seq_length=min_seq_length,\n",
    "                                  max_seq_length=max_seq_length):\n",
    "    print(progress)  # The pipeline is a generator function to inform the user about the current progress."
   ],
   "id": "e4fc5ae4bab03f07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Advanced Options 2: Precomputed embeddings file\n",
    "\n",
    "Another option is to use precomputed embeddings file, if you prefer that or have them already. Just make sure that the files include embeddings for all framework sequences and are stored by sequence hash, according to biotrainer standards."
   ],
   "id": "9c544f5ecf9fb136"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from biotrainer.autoeval import get_unique_framework_sequences\n",
    "\n",
    "_, per_residue_seqs, per_sequence_seqs = get_unique_framework_sequences(framework=\"dwt\",\n",
    "                                                                        min_seq_length=min_seq_length,\n",
    "                                                                        max_seq_length=max_seq_length)\n",
    "# per_residue_seqs and per_sequence_seqs are dictionaries mapping sequence hashes to BiotrainerSequenceRecord objects, use that hash as an id when storing your embeddings\n",
    "\n",
    "per_residue_path = Path()  # TODO Your per-residue embeddings path\n",
    "per_sequence_path = Path()  # TODO Your per-sequence embeddings path\n",
    "for progress in autoeval_pipeline(embedder_name=embedder_name,\n",
    "                                  framework=framework,\n",
    "                                  precomputed_per_residue_embeddings=per_residue_path,\n",
    "                                  precomputed_per_sequence_embeddings=per_sequence_path,\n",
    "                                  min_seq_length=min_seq_length,\n",
    "                                  max_seq_length=max_seq_length):\n",
    "    print(progress)  # The pipeline is a generator function to inform the user about the current progress."
   ],
   "id": "40388d81df59747",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
