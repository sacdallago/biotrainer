config:
  auto_resume: false
  batch_size: 128
  bootstrapping_iterations: 30
  cross_validation_config:
    method: hold_out
    choose_by: loss
  device: cpu
  disable_pytorch_compile: true
  dropout_rate: 0.25
  embedder_name: one_hot_encoding
  epsilon: 0.001
  external_writer: tensorboard
  ignore_file_inconsistencies: false
  input_file: 
    /home/sebie/PycharmProjects/biotrainerFork/examples/sequence_to_value/sequences.fasta
  learning_rate: 0.001
  limited_sample_size: -1
  log_dir: 
    /home/sebie/PycharmProjects/biotrainerFork/examples/sequence_to_value/output/FNN/one_hot_encoding
  loss_choice: mean_squared_error
  model_choice: FNN
  num_epochs: 200
  optimizer_choice: adam
  output_dir: /home/sebie/PycharmProjects/biotrainerFork/examples/sequence_to_value/output
  patience: 10
  protocol: sequence_to_value
  sanity_check: true
  save_split_ids: false
  seed: 42
  shuffle: true
  use_class_weights: false
  use_half_precision: false
  validate_input: true
database_type: Protein
derived_values:
  biotrainer_version: 1.0.0
  embeddings_file: 
    /home/sebie/PycharmProjects/biotrainerFork/examples/sequence_to_value/output/sequence_to_value/one_hot_encoding/reduced_embeddings_file_one_hot_encoding.h5
  model_hash: b2c305b6a505cc63
  n_classes: 1
  n_features: 21
  n_testing_ids: 2
  pipeline_elapsed_time: 0.5734401800000342
  pipeline_end_time: '2025-08-18T11:40:06.140618'
  pipeline_start_time: '2025-08-18T11:40:05.567178'
  training_elapsed_time: 0.3997226219998993
training_results:
  hold_out:
    n_training_ids: 1
    n_validation_ids: 1
    split_hyper_params: {}
    n_free_parameters: 737
    start_time: '2025-08-18T11:40:05.705991'
    end_time: '2025-08-18T11:40:06.104006'
    elapsed_time: 0.3980067299999064
    best_training_epoch_metrics:
      epoch: 120
      training:
        loss: 1359.240966796875
        mse: 1359.240966796875
        rmse: 36.86788558959961
        spearmans-corr-coeff: 0.0
      validation:
        loss: 1728.0814208984375
        mse: 1728.0814208984375
        rmse: 41.57019805908203
        spearmans-corr-coeff: 0.0
    training_loss:
      '0': 1406.0992431640625
      '1': 1405.425048828125
      '2': 1405.7825927734375
      '3': 1404.8507080078125
      '4': 1406.181884765625
      '5': 1399.8175048828125
      '6': 1405.1961669921875
      '7': 1398.473388671875
      '8': 1399.185546875
      '9': 1410.20703125
      '10': 1399.18359375
      '11': 1403.79833984375
      '12': 1400.305419921875
      '13': 1398.5672607421875
      '14': 1403.843505859375
      '15': 1405.5928955078125
      '16': 1399.0078125
      '17': 1399.284912109375
      '18': 1395.5877685546875
      '19': 1399.16650390625
      '20': 1398.7373046875
      '21': 1396.6942138671875
      '22': 1400.4141845703125
      '23': 1392.94091796875
      '24': 1401.09033203125
      '25': 1393.59228515625
      '26': 1395.02099609375
      '27': 1395.5413818359375
      '28': 1396.6160888671875
      '29': 1392.43896484375
      '30': 1392.4957275390625
      '31': 1401.914306640625
      '32': 1393.8360595703125
      '33': 1394.1622314453125
      '34': 1399.0594482421875
      '35': 1389.75341796875
      '36': 1398.4862060546875
      '37': 1393.9608154296875
      '38': 1397.7650146484375
      '39': 1388.064453125
      '40': 1388.49853515625
      '41': 1387.2479248046875
      '42': 1382.6990966796875
      '43': 1396.19921875
      '44': 1383.2786865234375
      '45': 1388.7347412109375
      '46': 1392.7716064453125
      '47': 1386.6375732421875
      '48': 1384.40234375
      '49': 1388.93408203125
      '50': 1381.619873046875
      '51': 1382.2296142578125
      '52': 1377.4539794921875
      '53': 1388.5621337890625
      '54': 1382.409423828125
      '55': 1387.3402099609375
      '56': 1392.1796875
      '57': 1374.2490234375
      '58': 1378.2733154296875
      '59': 1373.57373046875
      '60': 1388.1607666015625
      '61': 1381.036865234375
      '62': 1378.801025390625
      '63': 1372.3555908203125
      '64': 1382.554443359375
      '65': 1381.8126220703125
      '66': 1377.849853515625
      '67': 1370.0220947265625
      '68': 1377.0726318359375
      '69': 1377.3704833984375
      '70': 1387.0621337890625
      '71': 1371.1072998046875
      '72': 1379.677978515625
      '73': 1366.454345703125
      '74': 1377.518310546875
      '75': 1381.1854248046875
      '76': 1372.3095703125
      '77': 1372.429443359375
      '78': 1363.322265625
      '79': 1366.2135009765625
      '80': 1368.938720703125
      '81': 1376.5511474609375
      '82': 1369.677001953125
      '83': 1366.3514404296875
      '84': 1371.41796875
      '85': 1378.4735107421875
      '86': 1358.598876953125
      '87': 1366.8692626953125
      '88': 1372.748779296875
      '89': 1364.9803466796875
      '90': 1375.8314208984375
      '91': 1381.73974609375
      '92': 1361.3206787109375
      '93': 1354.3101806640625
      '94': 1353.7626953125
      '95': 1379.081787109375
      '96': 1363.27978515625
      '97': 1351.468017578125
      '98': 1362.6953125
      '99': 1363.5074462890625
      '100': 1370.4024658203125
      '101': 1356.2216796875
      '102': 1361.9583740234375
      '103': 1365.504150390625
      '104': 1360.0166015625
      '105': 1359.555419921875
      '106': 1364.2977294921875
      '107': 1353.344970703125
      '108': 1372.834716796875
      '109': 1355.23876953125
      '110': 1350.872314453125
      '111': 1370.3538818359375
      '112': 1341.9425048828125
      '113': 1341.2391357421875
      '114': 1359.4652099609375
      '115': 1345.4630126953125
      '116': 1354.464599609375
      '117': 1338.3092041015625
      '118': 1351.9681396484375
      '119': 1363.43896484375
      '120': 1359.240966796875
      '121': 1335.3155517578125
      '122': 1356.5400390625
      '123': 1342.7467041015625
      '124': 1354.692626953125
      '125': 1332.21044921875
      '126': 1365.47314453125
      '127': 1343.595703125
      '128': 1348.48681640625
      '129': 1346.4107666015625
      '130': 1341.588134765625
      '131': 1338.038818359375
    validation_loss:
      '0': 1785.6314697265625
      '1': 1788.6551513671875
      '2': 1789.989990234375
      '3': 1787.092529296875
      '4': 1789.3800048828125
      '5': 1788.8045654296875
      '6': 1786.2386474609375
      '7': 1788.9407958984375
      '8': 1788.8297119140625
      '9': 1784.8172607421875
      '10': 1784.2093505859375
      '11': 1784.561279296875
      '12': 1786.46240234375
      '13': 1790.2420654296875
      '14': 1781.4141845703125
      '15': 1784.7027587890625
      '16': 1783.962890625
      '17': 1784.244873046875
      '18': 1780.0140380859375
      '19': 1783.1708984375
      '20': 1780.7364501953125
      '21': 1780.93115234375
      '22': 1783.5181884765625
      '23': 1781.8531494140625
      '24': 1776.3740234375
      '25': 1781.53173828125
      '26': 1788.6842041015625
      '27': 1779.0050048828125
      '28': 1783.7147216796875
      '29': 1778.3360595703125
      '30': 1785.024169921875
      '31': 1775.4927978515625
      '32': 1776.2183837890625
      '33': 1776.6531982421875
      '34': 1774.722900390625
      '35': 1779.97119140625
      '36': 1770.03857421875
      '37': 1779.19775390625
      '38': 1778.5150146484375
      '39': 1777.24365234375
      '40': 1771.164794921875
      '41': 1773.9510498046875
      '42': 1775.09228515625
      '43': 1769.0986328125
      '44': 1781.5419921875
      '45': 1777.9351806640625
      '46': 1772.93603515625
      '47': 1769.3052978515625
      '48': 1768.4173583984375
      '49': 1777.23974609375
      '50': 1769.677490234375
      '51': 1775.0633544921875
      '52': 1770.3721923828125
      '53': 1772.4696044921875
      '54': 1777.6123046875
      '55': 1773.32568359375
      '56': 1772.897216796875
      '57': 1777.4169921875
      '58': 1766.6494140625
      '59': 1774.7769775390625
      '60': 1770.3875732421875
      '61': 1769.8023681640625
      '62': 1760.2945556640625
      '63': 1765.6707763671875
      '64': 1766.9912109375
      '65': 1761.836181640625
      '66': 1759.2117919921875
      '67': 1764.7252197265625
      '68': 1767.5509033203125
      '69': 1766.8995361328125
      '70': 1755.3460693359375
      '71': 1766.16162109375
      '72': 1755.1341552734375
      '73': 1765.7041015625
      '74': 1757.3211669921875
      '75': 1757.3604736328125
      '76': 1766.5242919921875
      '77': 1756.8021240234375
      '78': 1751.07568359375
      '79': 1756.960693359375
      '80': 1757.3367919921875
      '81': 1753.3494873046875
      '82': 1754.30517578125
      '83': 1759.87646484375
      '84': 1750.9425048828125
      '85': 1759.2117919921875
      '86': 1748.0579833984375
      '87': 1754.171875
      '88': 1747.3748779296875
      '89': 1770.6475830078125
      '90': 1759.27294921875
      '91': 1762.2847900390625
      '92': 1756.0714111328125
      '93': 1743.3140869140625
      '94': 1755.1533203125
      '95': 1751.9962158203125
      '96': 1754.01025390625
      '97': 1755.25537109375
      '98': 1749.3338623046875
      '99': 1747.979248046875
      '100': 1764.96142578125
      '101': 1746.1746826171875
      '102': 1738.131103515625
      '103': 1747.9189453125
      '104': 1736.93115234375
      '105': 1745.602783203125
      '106': 1753.50927734375
      '107': 1739.8309326171875
      '108': 1744.6131591796875
      '109': 1749.2330322265625
      '110': 1735.8016357421875
      '111': 1742.6322021484375
      '112': 1747.4779052734375
      '113': 1739.8707275390625
      '114': 1747.87109375
      '115': 1739.7978515625
      '116': 1750.955322265625
      '117': 1755.0810546875
      '118': 1734.3070068359375
      '119': 1738.388671875
      '120': 1728.0814208984375
      '121': 1750.01904296875
      '122': 1739.5360107421875
      '123': 1750.682373046875
      '124': 1734.2056884765625
      '125': 1735.2469482421875
      '126': 1732.97509765625
      '127': 1753.364501953125
      '128': 1733.389892578125
      '129': 1731.6556396484375
      '130': 1732.61962890625
      '131': 1730.0308837890625
test_results:
  test:
    metrics:
      loss: 1556.7738037109375
      mse: 1556.7738037109375
      rmse: 39.45597457885742
      spearmans-corr-coeff: 0.9999959468841553
    bootstrapping:
      results:
        mse:
          mean: 1604.13330078125
          lower: 135.125
          upper: 2978.0
        rmse:
          mean: 36.571876525878906
          lower: 11.625
          upper: 54.5625
        spearmans-corr-coeff:
          mean: 0.4333333373069763
          lower: 0.0
          upper: 1.0
      iterations: 30
      sample_size: 2
      confidence_level: 0.05
    test_baselines:
      random_model:
        results:
          mse:
            mean: 1638.8333740234375
            lower: 147.0
            upper: 3034.0
          rmse:
            mean: 37.040626525878906
            lower: 12.125
            upper: 55.09375
          spearmans-corr-coeff:
            mean: 0.4333333373069763
            lower: 0.0
            upper: 1.0
        iterations: 30
        sample_size: 2
        confidence_level: 0.05
      mean_only:
        results:
          mse:
            mean: 470.1499938964844
            lower: 315.0
            upper: 636.0
          rmse:
            mean: 21.50208282470703
            lower: 17.75
            upper: 25.21875
          spearmans-corr-coeff:
            mean: 0.0
            lower: 0.0
            upper: 0.0
        iterations: 30
        sample_size: 2
        confidence_level: 0.05
predictions: {}
