config:
  auto_resume: false
  batch_size: 128
  bootstrapping_iterations: 30
  cross_validation_config:
    method: hold_out
    choose_by: loss
  device: cpu
  disable_pytorch_compile: true
  dropout_rate: 0.25
  embedder_name: one_hot_encoding
  epsilon: 0.001
  external_writer: tensorboard
  ignore_file_inconsistencies: false
  input_file: 
    /home/sebie/PycharmProjects/biotrainerFork/examples/residues_to_value/sequences.fasta
  learning_rate: 0.001
  limited_sample_size: -1
  log_dir: 
    /home/sebie/PycharmProjects/biotrainerFork/examples/residues_to_value/output/LightAttention/one_hot_encoding
  loss_choice: mean_squared_error
  model_choice: LightAttention
  num_epochs: 200
  optimizer_choice: adam
  output_dir: /home/sebie/PycharmProjects/biotrainerFork/examples/residues_to_value/output
  patience: 10
  protocol: residues_to_value
  sanity_check: true
  save_split_ids: false
  seed: 42
  shuffle: true
  use_class_weights: false
  use_half_precision: false
  validate_input: true
database_type: Protein
derived_values:
  biotrainer_version: 1.0.0
  embeddings_file: 
    /home/sebie/PycharmProjects/biotrainerFork/examples/residues_to_value/output/residues_to_value/one_hot_encoding/embeddings_file_one_hot_encoding.h5
  model_hash: 5937b7e85feddbd3
  n_classes: 1
  n_features: 21
  n_testing_ids: 2
  pipeline_elapsed_time: 2.1990655019999394
  pipeline_end_time: '2025-07-02T18:25:01.178774'
  pipeline_start_time: '2025-07-02T18:24:58.979708'
  training_elapsed_time: 1.1985612949993083
training_results:
  hold_out:
    n_training_ids: 2
    n_validation_ids: 2
    split_hyper_params: {}
    n_free_parameters: 9453
    start_time: '2025-07-02T18:24:59.158445'
    end_time: '2025-07-02T18:25:00.354747'
    elapsed_time: 1.1962917750006454
    best_training_epoch_metrics:
      epoch: 138
      training:
        loss: 828.9908447265625
        mse: 828.9908447265625
        rmse: 28.79220199584961
        spearmans-corr-coeff: 0.9999959468841553
      validation:
        loss: 1221.713134765625
        mse: 1221.713134765625
        rmse: 34.9530143737793
        spearmans-corr-coeff: -0.9999959468841553
    training_loss:
      '0': 1255.24560546875
      '1': 1290.302978515625
      '2': 1212.64111328125
      '3': 1243.0126953125
      '4': 1220.019287109375
      '5': 1211.22998046875
      '6': 1191.453125
      '7': 1183.5728759765625
      '8': 1211.7305908203125
      '9': 1168.264404296875
      '10': 1166.84814453125
      '11': 1165.31982421875
      '12': 1135.782958984375
      '13': 1145.9609375
      '14': 1125.04150390625
      '15': 1102.854248046875
      '16': 1102.490966796875
      '17': 1103.1787109375
      '18': 1111.6458740234375
      '19': 1078.545166015625
      '20': 1080.334716796875
      '21': 1089.251708984375
      '22': 1086.9464111328125
      '23': 1077.67431640625
      '24': 1063.3897705078125
      '25': 1093.551025390625
      '26': 1047.1295166015625
      '27': 1079.425537109375
      '28': 1056.034423828125
      '29': 1046.854736328125
      '30': 1085.650146484375
      '31': 1058.63623046875
      '32': 1058.3336181640625
      '33': 1041.19189453125
      '34': 1050.778076171875
      '35': 1041.31640625
      '36': 1028.9111328125
      '37': 1031.522705078125
      '38': 1033.4503173828125
      '39': 1047.975341796875
      '40': 1038.4996337890625
      '41': 1020.628662109375
      '42': 1047.482421875
      '43': 1013.79052734375
      '44': 1065.606689453125
      '45': 1035.235595703125
      '46': 1030.4637451171875
      '47': 1012.5257568359375
      '48': 1006.2886352539062
      '49': 1013.4754028320312
      '50': 1023.478515625
      '51': 1011.7257080078125
      '52': 988.31982421875
      '53': 996.9860229492188
      '54': 1029.6484375
      '55': 991.895263671875
      '56': 970.15869140625
      '57': 1001.07470703125
      '58': 1019.9356689453125
      '59': 979.483154296875
      '60': 1024.5828857421875
      '61': 1051.8369140625
      '62': 980.213623046875
      '63': 1024.68505859375
      '64': 970.4786376953125
      '65': 993.0091552734375
      '66': 969.1690673828125
      '67': 1001.717529296875
      '68': 985.5048828125
      '69': 977.7470703125
      '70': 995.62451171875
      '71': 949.7020263671875
      '72': 959.8792114257812
      '73': 967.98193359375
      '74': 956.4519653320312
      '75': 933.448486328125
      '76': 970.9212036132812
      '77': 945.5419921875
      '78': 946.33544921875
      '79': 1004.6881103515625
      '80': 995.1953125
      '81': 955.5704345703125
      '82': 949.6591796875
      '83': 951.1939697265625
      '84': 945.4459228515625
      '85': 930.871337890625
      '86': 933.4260864257812
      '87': 894.6015014648438
      '88': 920.2196044921875
      '89': 940.0985107421875
      '90': 947.4622802734375
      '91': 963.2354125976562
      '92': 928.5330810546875
      '93': 938.3232421875
      '94': 911.871826171875
      '95': 919.12109375
      '96': 930.7018432617188
      '97': 924.064208984375
      '98': 907.59619140625
      '99': 913.327880859375
      '100': 906.09130859375
      '101': 931.58837890625
      '102': 903.2552490234375
      '103': 893.8101806640625
      '104': 864.0537109375
      '105': 880.6927490234375
      '106': 897.80419921875
      '107': 889.248291015625
      '108': 886.0054931640625
      '109': 874.2965087890625
      '110': 838.3079223632812
      '111': 861.773681640625
      '112': 899.3262939453125
      '113': 857.3759765625
      '114': 852.3380737304688
      '115': 836.9420166015625
      '116': 843.0422973632812
      '117': 867.1311645507812
      '118': 833.5645751953125
      '119': 889.999755859375
      '120': 858.63671875
      '121': 853.901611328125
      '122': 870.1732177734375
      '123': 844.092041015625
      '124': 815.7023315429688
      '125': 815.5360107421875
      '126': 841.1463623046875
      '127': 821.064453125
      '128': 851.82763671875
      '129': 835.7537231445312
      '130': 809.0784912109375
      '131': 832.35302734375
      '132': 786.9117431640625
      '133': 822.75830078125
      '134': 818.9312133789062
      '135': 777.208984375
      '136': 815.6551513671875
      '137': 783.7476806640625
      '138': 828.9908447265625
      '139': 815.782470703125
      '140': 792.4193725585938
      '141': 816.9677734375
      '142': 805.3563232421875
      '143': 780.7454223632812
      '144': 783.7940063476562
      '145': 801.78955078125
      '146': 753.6834106445312
      '147': 784.3497314453125
      '148': 789.3143310546875
      '149': 751.4868774414062
    validation_loss:
      '0': 1828.7440185546875
      '1': 1815.5518798828125
      '2': 1831.38818359375
      '3': 1781.7041015625
      '4': 1775.404541015625
      '5': 1808.735595703125
      '6': 1770.417236328125
      '7': 1709.333984375
      '8': 1727.416259765625
      '9': 1780.552490234375
      '10': 1709.980224609375
      '11': 1685.564208984375
      '12': 1710.4521484375
      '13': 1713.5068359375
      '14': 1700.807373046875
      '15': 1704.63525390625
      '16': 1638.578125
      '17': 1680.1160888671875
      '18': 1702.67333984375
      '19': 1692.7874755859375
      '20': 1660.69482421875
      '21': 1719.28515625
      '22': 1663.48876953125
      '23': 1649.6572265625
      '24': 1627.88818359375
      '25': 1678.4189453125
      '26': 1611.776123046875
      '27': 1599.963134765625
      '28': 1607.158203125
      '29': 1596.3629150390625
      '30': 1586.224853515625
      '31': 1600.12646484375
      '32': 1581.717041015625
      '33': 1598.528564453125
      '34': 1595.4267578125
      '35': 1597.0272216796875
      '36': 1621.6339111328125
      '37': 1549.396240234375
      '38': 1581.691162109375
      '39': 1558.3094482421875
      '40': 1570.5765380859375
      '41': 1559.295166015625
      '42': 1592.0145263671875
      '43': 1586.538330078125
      '44': 1561.032958984375
      '45': 1543.2916259765625
      '46': 1531.7762451171875
      '47': 1581.294677734375
      '48': 1561.2255859375
      '49': 1560.6546630859375
      '50': 1524.40869140625
      '51': 1530.254638671875
      '52': 1553.5994873046875
      '53': 1530.2177734375
      '54': 1524.158935546875
      '55': 1545.8800048828125
      '56': 1519.236572265625
      '57': 1521.915283203125
      '58': 1528.679931640625
      '59': 1491.55029296875
      '60': 1532.6964111328125
      '61': 1531.401611328125
      '62': 1524.081787109375
      '63': 1564.2945556640625
      '64': 1542.88623046875
      '65': 1537.755859375
      '66': 1491.2166748046875
      '67': 1472.762939453125
      '68': 1513.447509765625
      '69': 1503.3980712890625
      '70': 1533.2962646484375
      '71': 1527.6978759765625
      '72': 1477.4840087890625
      '73': 1513.060302734375
      '74': 1480.572998046875
      '75': 1522.488037109375
      '76': 1512.67578125
      '77': 1521.517333984375
      '78': 1460.834716796875
      '79': 1483.897216796875
      '80': 1482.997314453125
      '81': 1496.105224609375
      '82': 1454.7138671875
      '83': 1441.302978515625
      '84': 1476.583251953125
      '85': 1500.6090087890625
      '86': 1458.255859375
      '87': 1480.3902587890625
      '88': 1467.1978759765625
      '89': 1438.1297607421875
      '90': 1434.292236328125
      '91': 1447.116455078125
      '92': 1436.77099609375
      '93': 1451.736083984375
      '94': 1429.572998046875
      '95': 1449.1512451171875
      '96': 1433.7843017578125
      '97': 1464.22314453125
      '98': 1408.1187744140625
      '99': 1416.84033203125
      '100': 1378.26953125
      '101': 1476.888671875
      '102': 1432.384033203125
      '103': 1451.600341796875
      '104': 1368.4921875
      '105': 1408.6373291015625
      '106': 1429.80810546875
      '107': 1376.212890625
      '108': 1439.916015625
      '109': 1421.8843994140625
      '110': 1398.5667724609375
      '111': 1389.46630859375
      '112': 1470.09375
      '113': 1348.546875
      '114': 1347.87646484375
      '115': 1332.46240234375
      '116': 1357.438720703125
      '117': 1320.223388671875
      '118': 1385.796875
      '119': 1380.913330078125
      '120': 1378.58544921875
      '121': 1333.3385009765625
      '122': 1324.715576171875
      '123': 1354.4677734375
      '124': 1395.97802734375
      '125': 1315.55322265625
      '126': 1330.18212890625
      '127': 1413.66357421875
      '128': 1352.4952392578125
      '129': 1326.477783203125
      '130': 1286.3974609375
      '131': 1331.891845703125
      '132': 1315.5562744140625
      '133': 1309.791015625
      '134': 1256.4798583984375
      '135': 1318.6463623046875
      '136': 1315.1859130859375
      '137': 1300.325927734375
      '138': 1221.713134765625
      '139': 1266.51171875
      '140': 1307.9351806640625
      '141': 1262.55517578125
      '142': 1258.76611328125
      '143': 1277.48486328125
      '144': 1362.096435546875
      '145': 1244.5992431640625
      '146': 1261.3839111328125
      '147': 1336.668701171875
      '148': 1225.6412353515625
      '149': 1268.3544921875
test_results:
  test:
    metrics:
      loss: 1138.50439453125
      mse: 1138.50439453125
      rmse: 33.741729736328125
      spearmans-corr-coeff: 0.9999959468841553
    bootstrapping:
      results:
        mse:
          mean: 1176.0
          error: 1675.0
        rmse:
          mean: 30.109375
          error: 32.78125
        spearmans-corr-coeff:
          mean: 0.433349609375
          error: 0.98779296875
      iterations: 30
      sample_size: 2
      confidence_level: 0.05
    test_baselines:
      random_model:
        results:
          mse:
            mean: 1642.0
            error: 2168.0
          rmse:
            mean: 37.0625
            error: 32.5625
          spearmans-corr-coeff:
            mean: -0.433349609375
            error: 0.98779296875
        iterations: 30
        sample_size: 2
        confidence_level: 0.05
      mean_only:
        results:
          mse:
            mean: 461.5
            error: 0.0
          rmse:
            mean: 21.484375
            error: 0.0
          spearmans-corr-coeff:
            mean: 0.0
            error: 0.0
        iterations: 30
        sample_size: 2
        confidence_level: 0.05
predictions: {}
