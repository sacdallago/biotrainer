import onnx
import torch
import numpy as np

from pathlib import Path
from numpy import ndarray
from transformers import PreTrainedTokenizer
from typing import Union, List, Any, Generator

from .custom_tokenizer import CustomTokenizer
from .embedder_interfaces import EmbedderWithFallback

from ..utilities import is_device_cuda, is_device_mps, get_device_memory, get_logger

logger = get_logger(__name__)


class OrtSessionWrapper:
    def __init__(self, session):
        self.session = session
        self._is_cpu_provider = self._check_if_cpu_provider()

    def to(self, device):
        return self

    def _check_if_cpu_provider(self):
        """Check if the primary execution provider is CPU"""
        # Get the active provider
        # (first provider in the list is the primary one being used)
        providers = self.session.get_providers()
        return providers[0] == 'CPUExecutionProvider'

    def run(self, output_names, input_feed, run_options=None):
        # For CPU sessions, use strategic processing
        if self._is_cpu_provider:
            return self._run_with_memory_safety(output_names, input_feed, run_options)
        return self.session.run(output_names, input_feed=input_feed, run_options=run_options)

    def _run_with_memory_safety(self, output_names, input_feed, run_options=None):
        """Run inference with CPU memory safety considerations"""
        input_size = self._estimate_input_size(input_feed)  # GB
        safety_factor = 0.6  # Conservative safety factor considering large model architectures
        max_memory = get_device_memory(device=torch.device('cpu')) * safety_factor

        if input_size > max_memory:
            raise RuntimeError(f"Failed to use ONNX CPU Runtime to embed input of size {input_size}, "
                               f"because it exceeds the memory limit of {max_memory} GB.")

        return self.session.run(output_names, input_feed=input_feed, run_options=run_options)

    @staticmethod
    def _estimate_input_size(input_feed) -> float:
        """Roughly estimate input size in bytes"""
        minimum_size = 4.0   # At least 4GB for any inference
        gb_factor = (1024 ** 3)
        mb_factor = (1024 ** 2)

        # 1. Base input size
        sequence_length = 1000  # Reasonable default value
        for key, tensor in input_feed.items():
            if isinstance(tensor, list):
                if key == 'input_ids':
                    sequence_length += len(tensor[0])

        input_size = sequence_length * mb_factor  # Estimating 1MB/residue

        # 2. Model complexity factor - Transformer models use O(nÂ²) memory for attention
        complexity_factor = 1.5

        # 3. Model base overhead (the ONNX model itself plus runtime overhead)
        model_base_overhead_gb = 2.0  # Assume usually about 2GB for model weights and runtime

        # Calculate total memory estimate
        estimated_memory_gb = (
                (input_size / gb_factor) * complexity_factor
                + model_base_overhead_gb
        )

        return max(estimated_memory_gb, minimum_size)


class OnnxEmbedder(EmbedderWithFallback):

    def __init__(self, onnx_path: Path, tokenizer: PreTrainedTokenizer, device: Union[str, torch.device]):
        try:
            import onnxruntime as ort
            self._onnxruntime = ort
            self._check_onnxruntime_gpu()
        except ImportError:
            raise Exception("No onnxruntime in current environment found! Please install one via poetry extras first!")

        self.name = f"onnx-{onnx_path.name.replace('.onnx', '')}"
        self._onnx_path = onnx_path
        self._device = device
        self._model = self._load_model()

        self._tokenizer = tokenizer
        if isinstance(tokenizer, CustomTokenizer):
            self._preprocessing_strategy = tokenizer.preprocessing_strategy

    def _check_onnxruntime_gpu(self):
        if torch.cuda.is_available():
            if 'CUDAExecutionProvider' not in self._onnxruntime.get_available_providers():
                logger.info("CUDA is available but onnxruntime-gpu is not installed. "
                      "Install it with: poetry install -E onnx-gpu")
        if torch.mps.is_available():
            if 'CoreMLExecutionProvider' not in self._onnxruntime.get_available_providers():
                logger.info("MPS is available but onnxruntime-coreml is not installed. "
                      "Install it with: poetry install -E onnx-mac")

    def _load_model(self) -> OrtSessionWrapper:
        # Create ONNX Runtime session
        onnx_model = onnx.load(self._onnx_path)
        onnx.checker.check_model(onnx_model)

        if is_device_cuda(self._device):
            ep_list = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        elif is_device_mps(self._device):
            ep_list = ['CoreMLExecutionProvider', 'CPUExecutionProvider']
        else:
            ep_list = ['CPUExecutionProvider']

        ort_session = self._onnxruntime.InferenceSession(self._onnx_path, providers=ep_list)

        return OrtSessionWrapper(ort_session)

    def _get_fallback_model(self) -> OrtSessionWrapper:
        onnx_model = onnx.load(self._onnx_path)
        onnx.checker.check_model(onnx_model)

        ep_list = ['CPUExecutionProvider']

        ort_session = self._onnxruntime.InferenceSession(self._onnx_path, providers=ep_list)
        return OrtSessionWrapper(ort_session)

    def _embed_single(self, sequence: str) -> ndarray:
        [embedding] = self._embed_batch([sequence])
        return embedding

    def _embed_batch_implementation(self, batch: List[str], model: Any) -> Generator[ndarray, None, None]:
        # Tokenize the batch
        encoded = self._tokenizer.batch_encode_plus(
            batch,
            add_special_tokens=True,
            padding="longest",
        )

        # Get input IDs and attention mask
        input_ids = encoded["input_ids"]
        attention_mask = encoded["attention_mask"]

        # Run inference with ONNX model
        ort_inputs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask
        }

        # Get embeddings
        try:
            embeddings = model.run(None, ort_inputs)[0]
        except Exception as e:
            raise RuntimeError(e)

        # Process each sequence in the batch
        for seq_num in range(len(embeddings)):
            input_id = input_ids[seq_num]

            # Remove special tokens
            if hasattr(self._tokenizer, "get_special_tokens_mask"):
                special_tokens_mask = self._tokenizer.get_special_tokens_mask(
                    input_id,
                    already_has_special_tokens=True
                )
                indices_to_remove = [
                    index for index, mask in enumerate(special_tokens_mask)
                    if mask != 0
                ]
                embedding = np.delete(embeddings[seq_num], indices_to_remove, axis=0)
            else:
                embedding = embeddings[seq_num]

            yield embedding
