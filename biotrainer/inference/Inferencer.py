from __future__ import annotations

import os

import torch
import tempfile
import numpy as np

from ruamel import yaml
from pathlib import Path
from copy import deepcopy
from torch.utils.data import DataLoader
from typing import Union, Optional, Dict, Iterable, Tuple, Any

from ..losses import get_loss
from ..models import get_model
from ..solvers import get_solver
from ..optimizers import get_optimizer
from ..trainers import revert_mappings
from ..datasets import get_dataset, get_collate_function
from ..utilities import get_device, seed_all, DatasetSample


class Inferencer:

    def __init__(
            self,
            # Constant parameters for all split solvers
            protocol: str,
            embedder_name: str,
            # Optional constant parameters
            class_int_to_string: Optional[Dict[int, str]] = None,
            device: Union[None, str, torch.device] = None,
            # Everything else
            **kwargs
    ):
        self.protocol = protocol
        self.device = get_device(device)
        self.embedder_name = embedder_name
        self.class_int2str = class_int_to_string
        self.collate_function = get_collate_function(protocol)

        self.solvers_and_loaders_by_split = self._create_solvers_and_loaders_by_split(**kwargs)
        print(f"Got {len(self.solvers_and_loaders_by_split.keys())} split(s): "
              f"{', '.join(self.solvers_and_loaders_by_split.keys())}")

    @classmethod
    def create_from_out_file(cls, out_file_path: str,
                             automatic_path_correction: bool = True) -> Tuple[Inferencer, Dict[str, Any]]:
        """
        Create the inferencer object from the out.yml file generated by biotrainer.
        Reads the out.yml file without the split ids which would blow up the file unnecessarily.

        :param out_file_path: Path to out.yml file generated by biotrainer
        :param automatic_path_correction: If True, the method tries to correct the log_dir path if it is not found.
                                          In this case, checkpoints are searched at
                                          out_file_path/model_choice/embedder_name

        :return: Tuple with Inferencer object configured with the output variables from the out.yml file,
                 and the output variables as dict
        """

        print(f"Reading {out_file_path}..")
        with tempfile.TemporaryDirectory() as tmp_dir_name:
            tmp_output_path = tmp_dir_name + "/tmp_output.yml"
            with open(out_file_path, "r") as output_file, open(tmp_output_path, "w") as tmp_output_file:
                ids_list = False
                for line in output_file.readlines():
                    if line.strip() == "training_ids:" or line.strip() == "validation_ids:":
                        ids_list = True
                        continue
                    elif ids_list and ("-" in line and ":" not in line):
                        continue
                    else:
                        ids_list = False
                    if not ids_list:
                        tmp_output_file.write(line)

            with open(tmp_output_path, "r") as tmp_output_file:
                output_vars = yaml.load(tmp_output_file, Loader=yaml.RoundTripLoader)

        if automatic_path_correction:
            log_dir = output_vars["log_dir"]
            log_dir_path = Path(log_dir)
            if not log_dir_path.exists():
                # Expect checkpoints to be in output/model_choice/embedder_name
                checkpoints_path = output_vars["model_choice"] + "/" + output_vars["embedder_name"]
                new_log_dir_path = Path("/".join(
                    [directory for directory in out_file_path.split("/")[0:-1]])) / Path(checkpoints_path)

                if not new_log_dir_path.exists():
                    print(f"Could not automatically correct the checkpoint file paths! "
                          f"Tried: {str(new_log_dir_path)} but it does not exist.")
                elif len(os.listdir(str(new_log_dir_path))) == 0:
                    print(f"Found corrected path ({str(new_log_dir_path)}), but it does not contain any files!")
                else:
                    print(f"Reading checkpoint(s) from directory: {new_log_dir_path}..")
                    output_vars["log_dir"] = new_log_dir_path

        return cls(**output_vars), output_vars

    def _create_solvers_and_loaders_by_split(self, **kwargs) -> Dict[str, Tuple[Any, Any]]:
        result_dict = {}
        splits = kwargs["split_results"].keys()
        for split in splits:
            # Ignore average or best result
            if "average" in split or "best" in split:
                continue
            split_config = deepcopy(kwargs)
            for key, value in kwargs["split_results"][split]["split_hyper_params"].items():
                split_config[key] = value

            # Positional arguments
            model_choice = split_config.pop("model_choice")
            n_classes = split_config.pop("n_classes")
            n_features = split_config.pop("n_features")
            loss_choice = split_config.pop("loss_choice")
            optimizer_choice = split_config.pop("optimizer_choice")
            learning_rate = split_config.pop("learning_rate")
            log_dir = split_config.pop("log_dir")

            model = get_model(protocol=self.protocol, model_choice=model_choice,
                              n_classes=n_classes, n_features=n_features,
                              **split_config
                              )
            loss_function = get_loss(protocol=self.protocol, loss_choice=loss_choice,
                                     device=self.device,
                                     **split_config
                                     )
            optimizer = get_optimizer(protocol=self.protocol, optimizer_choice=optimizer_choice,
                                      model_parameters=model.parameters(), learning_rate=learning_rate,
                                      **split_config
                                      )

            solver = get_solver(protocol=self.protocol, name=split, network=model, optimizer=optimizer,
                                loss_function=loss_function, device=self.device, log_dir=log_dir,
                                num_classes=n_classes)
            solver.load_checkpoint(resume_training=False)

            def dataloader_function(dataset):
                return DataLoader(dataset=dataset, batch_size=split_config["batch_size"],
                                  shuffle=False, drop_last=False,
                                  collate_fn=self.collate_function)

            result_dict[split] = (solver, dataloader_function)
        return result_dict

    def _load_solver_and_dataloader(self, embeddings: Union[Iterable, Dict],
                                    split_name, targets: Optional[Iterable] = None):
        if split_name not in self.solvers_and_loaders_by_split.keys():
            raise Exception(f"Unknown split_name {split_name} for given configuration!")

        if isinstance(embeddings, Dict):
            embeddings_dict = embeddings
        else:
            embeddings_dict = {str(idx): embedding for idx, embedding in enumerate(embeddings)}

        solver, loader = self.solvers_and_loaders_by_split[split_name]
        dataset = get_dataset(self.protocol, samples=[
            DatasetSample(seq_id, torch.tensor(np.array(embedding)),
                          torch.empty(1) if not targets else torch.tensor(np.array(targets[idx])))
            for idx, (seq_id, embedding) in enumerate(embeddings_dict.items())
        ])
        dataloader = loader(dataset)
        return solver, dataloader

    def from_embeddings(self, embeddings: Union[Iterable, Dict], targets: Optional[Iterable] = None,
                        split_name: str = "hold_out",
                        include_probabilities: bool = False) -> Dict[str, Union[Dict, str, int, float]]:
        """
        Calculate predictions from embeddings.

        :param embeddings: Iterable or dictionary containing the input embeddings to predict on.
        :param targets: Iterable that contains the targets to calculate metrics
        :param split_name: Name of the split to use for prediction. Default is "hold_out".
        :param include_probabilities: If True, the probabilities used to predict classes are also reported.
                                      Is only useful for classification tasks, otherwise the "probabilities" are the
                                      same as the predictions.
        :return: Dictionary containing the following sub-dictionaries:
                 - 'metrics': Calculated metrics if 'targets' are given, otherwise 'None'.
                 - 'mapped_predictions': Class or value prediction from the given embeddings.
                 - 'mapped_probabilities': Probabilities for classification tasks if include_probabilities is True.
                 Predictions and probabilities are either 'mapped' to keys from an embeddings dict or indexes if
                 embeddings are given as a list.
        """
        solver, dataloader = self._load_solver_and_dataloader(embeddings, split_name, targets)

        inference_dict = solver.inference(dataloader, calculate_test_metrics=targets is not None)
        predictions = inference_dict["mapped_predictions"]

        # For class predictions, revert from int (model output) to str (class name)
        inference_dict["mapped_predictions"] = revert_mappings(protocol=self.protocol, test_predictions=predictions,
                                                               class_int2str=self.class_int2str)

        if not include_probabilities:
            return {k: v for k, v in inference_dict.items() if k != "mapped_probabilities"}
        else:
            return inference_dict

    def from_embeddings_with_monte_carlo_dropout(self, embeddings: Union[Iterable, Dict],
                                                 split_name: str = "hold_out",
                                                 n_forward_passes: int = 30,
                                                 confidence_level: float = 0.05,
                                                 seed: int = 42) -> Dict:
        """
        Calculate predictions by using Monte Carlo dropout.
        Only works if the model has at least one dropout layer employed.
        Method to quantify the uncertainty within the model.

        :param embeddings: Iterable or dictionary containing the input embeddings to predict on.
        :param split_name: Name of the split to use for prediction. Default is "hold_out".
        :param n_forward_passes: Number of times to repeat the prediction calculation
                                with different dropout nodes enabled.
        :param confidence_level: Confidence level for the result confidence intervals. Default is 0.05,
                                which corresponds to a 95% percentile.
        :param seed: Seed to use for the dropout predictions

        :return: Dictionary containing with keys that will either be taken from the embeddings dict or
         represent the indexes if embeddings are given as a list. Contains the following values for each key:
                 - 'prediction': Class or value prediction based on the mean over `n_forward_passes` forward passes.
                 - 'mcd_mean': Average over `n_forward_passes` forward passes for each class.
                 - 'mcd_lower_bound': Lower bound of the confidence interval using a normal distribution with the given
                                      confidence level.
                 - 'mcd_upper_bound': Upper bound of the confidence interval using a normal distribution with the given
                                      confidence level.
        """

        # Necessary because dropout layer have a random part by design
        seed_all(seed)

        solver, dataloader = self._load_solver_and_dataloader(embeddings, split_name)

        predictions = solver.inference_monte_carlo_dropout(dataloader=dataloader,
                                                           n_forward_passes=n_forward_passes,
                                                           confidence_level=confidence_level)["mapped_predictions"]

        # For class predictions, revert from int (model output) to str (class name)
        if "residue_" in self.protocol:
            for seq_id, prediction_list in predictions.items():
                for prediction_dict in prediction_list:
                    prediction_dict["prediction"] = list(revert_mappings(protocol=self.protocol,
                                                                         test_predictions={
                                                                             seq_id: prediction_dict["prediction"]
                                                                         },
                                                                         class_int2str=self.class_int2str).values())[0]
        else:
            for seq_id, prediction_dict in predictions.items():
                prediction_dict["prediction"] = list(revert_mappings(protocol=self.protocol,
                                                                     test_predictions={
                                                                         seq_id: prediction_dict["prediction"]
                                                                     },
                                                                     class_int2str=self.class_int2str).values())[0]

        return predictions
